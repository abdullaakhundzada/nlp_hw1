# Azerbaijani NLP Processing Tool ðŸ‡¦ðŸ‡¿

A comprehensive NLP toolkit for processing Azerbaijani text, implementing tokenization, Heaps' Law analysis, Byte Pair Encoding, sentence segmentation, and spell checking with both Levenshtein distance and weighted edit distance using confusion matrices.

## Project Structure

```
.
â”œâ”€â”€ data                                                                     
â”‚Â Â  â””â”€â”€ all_contents
â”œâ”€â”€ setup.py
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ data_loader.py              # Data loading and preprocessing  
â”‚Â Â  â”œâ”€â”€ tokenization.py             # Task 1: Tokenization and frequency analysis
â”‚Â Â  â”œâ”€â”€ heaps_law.py                # Task 2: Heaps' Law analysis
â”‚Â Â  â”œâ”€â”€ bpe.py                      # Task 3: Byte Pair Encoding
â”‚Â Â  â”œâ”€â”€ sentence_segmentation.py    # Task 4: Sentence segmentation
â”‚Â Â  â”œâ”€â”€ spell_checking.py           # Task 5: Spell checking (Levenshtein)
â”‚Â Â  â””â”€â”€ extra_weighted.py           # Extra: Weighted edit distance
â”œâ”€â”€ main.py                         # Main execution script
â”œâ”€â”€ streamlit_app.py                # Streamlit UI application
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Your Data

Place your Azerbaijani JSON files in a folder. Each JSON file should contain a list of dictionaries with `page_number` and `content` keys:

```json
[
  {
    "page_number": 1,
    "content": "<text>"
  },
  {
    "page_number": 2,
    "content": "<text>"
  }
]
```

### 3. Run All Tasks

**With your own data:**
```bash
python main.py --data /path/to/your/data --output ./outputs
```

**With sample data (for testing):**
```bash
python main.py --sample --output ./outputs
```

### 4. Launch the UI

```bash
streamlit run streamlit_app.py
```

The UI will open in your browser at `http://localhost:8501`

## Tasks Overview

### Task 1: Tokenization & Frequency Analysis (15%)
- Tokenizes Azerbaijani text
- Calculates token and type frequencies
- Computes type-token ratio
- Outputs:
  - `vocabulary.pkl` - Saved vocabulary
  - `vocabulary.json` - Top 100 tokens with frequencies
  - `task1_report.txt` - Detailed statistics

### Task 2: Heaps' Law Analysis (15%)
- Tests Heaps' Law: V(n) = k Ã— n^Î²
- Finds optimal k and Î² parameters
- Generates vocabulary growth visualization
- Outputs:
  - `heaps_law_params.json` - k, Î², RÂ² values
  - `heaps_law_plot.png` - Visualization
  - `task2_report.txt` - Analysis report

### Task 3: Byte Pair Encoding (15%)
- Implements BPE algorithm for subword tokenization
- Learns merge operations from corpus
- Supports encoding/decoding
- Outputs:
  - `bpe_model.pkl` - Trained BPE model
  - `bpe_model.json` - Model statistics
  - `task3_report.txt` - Sample encodings

### Task 4: Sentence Segmentation (15%)
- Azerbaijani-aware sentence boundary detection
- Handles abbreviations and titles
- Supports complex punctuation
- Outputs:
  - `sentence_segmenter.pkl` - Segmenter model
  - `sentences.json` - All segmented sentences
  - `task4_report.txt` - Statistics

### Task 5: Spell Checker - Levenshtein Distance (15%)
- Classic spell checking using edit distance
- Provides top-N correction suggestions
- Ranks by distance and frequency
- Outputs:
  - `spell_checker.pkl` - Spell checker model
  - `spell_checker.json` - Vocabulary statistics
  - `spell_test_pairs.json` - Test cases
  - `task5_report.txt` - Evaluation results

### Extra Task: Weighted Edit Distance (20%)
- Advanced spell checking with character confusion matrix
- Learns common character substitution patterns
- Lower weights for common confusions (É™â†”a, Ä±â†”i, etc.)
- Outputs:
  - `weighted_spell_checker.pkl` - Weighted model
  - `weighted_spell_checker_confusion.pkl` - Confusion matrix
  - `extra_task_report.txt` - Confusion patterns

## Streamlit UI Features

The interactive UI provides:

### Overview Page
- Dataset statistics
- Summary of all task results
- Quick metrics dashboard

### Task 1: Tokenization
- Frequency distribution visualization
- Top tokens chart
- Interactive tokenization testing

### Task 2: Heaps' Law
- Formula display: V(n) = k Ã— n^Î²
- Parameter values (k, Î², RÂ²)
- Vocabulary growth plots
- Interpretation of Î² value

### Task 3: BPE
- Sample merge operations
- Vocabulary statistics
- Interactive encoding/decoding
- Token count comparison

### Task 4: Segmentation
- Sample segmented sentences
- Interactive sentence boundary detection
- Handles abbreviations correctly

### Task 5: Spell Checker
- Interactive spell checking
- Top-N suggestions with distances
- Batch text correction
- Edit distance configuration

### Extra: Weighted Edit Distance
- Character confusion matrix display
- Weighted vs. regular comparison
- Interactive weighted spell checking
- Confusion pattern analysis

## Output Files

All outputs are saved to `./outputs/` (or your specified directory):

```
outputs/
â”œâ”€â”€ vocabulary.pkl                      # Task 1
â”œâ”€â”€ vocabulary.json
â”œâ”€â”€ task1_report.txt
â”œâ”€â”€ heaps_law_params.json              # Task 2
â”œâ”€â”€ heaps_law_plot.png
â”œâ”€â”€ task2_report.txt
â”œâ”€â”€ bpe_model.pkl                      # Task 3
â”œâ”€â”€ bpe_model.json
â”œâ”€â”€ task3_report.txt
â”œâ”€â”€ sentence_segmenter.pkl             # Task 4
â”œâ”€â”€ sentences.json
â”œâ”€â”€ task4_report.txt
â”œâ”€â”€ spell_checker.pkl                  # Task 5
â”œâ”€â”€ spell_checker.json
â”œâ”€â”€ spell_test_pairs.json
â”œâ”€â”€ task5_report.txt
â”œâ”€â”€ weighted_spell_checker.pkl         # Extra Task
â”œâ”€â”€ weighted_spell_checker_confusion.pkl
â”œâ”€â”€ weighted_spell_checker_confusion.json
â”œâ”€â”€ extra_task_report.txt
â””â”€â”€ summary.json                       # Overall summary
```

## Module Usage Examples

### Using Tokenizer
```python
from task1_tokenization import Tokenizer

tokenizer = Tokenizer()
stats = tokenizer.process_corpus("AzÉ™rbaycan dili")
print(f"Tokens: {stats['total_tokens']}")
print(f"Types: {stats['total_types']}")
```

### Using BPE
```python
from task3_bpe import BPETokenizer

bpe = BPETokenizer(num_merges=500)
bpe.train(corpus)

encoded = bpe.encode("AzÉ™rbaycan")
decoded = bpe.decode(encoded)
print(f"Encoded: {encoded}")
```

### Using Spell Checker
```python
from task5_spell_checker import SpellChecker

checker = SpellChecker()
checker.build_vocabulary(corpus)

suggestions = checker.correct_word("azrbaycan", max_distance=2)
for word, dist, freq in suggestions:
    print(f"{word} (distance: {dist}, frequency: {freq})")
```

### Using Weighted Spell Checker
```python
from task_extra_weighted import WeightedSpellChecker, ConfusionMatrix

confusion = ConfusionMatrix()
checker = WeightedSpellChecker(confusion_matrix=confusion)
checker.build_vocabulary(corpus)

suggestions = checker.correct_word("azarbaycan", max_distance=2.0)
```

## Command Line Options

### Main Pipeline
```bash
python main.py --help
```

Options:
- `--data PATH`: Path to data folder containing JSON files
- `--output PATH`: Path to output folder (default: ./outputs)
- `--sample`: Create and use sample Azerbaijani data for testing

### Streamlit UI
```bash
streamlit run streamlit_app.py
```

## Report Generation

The tool automatically generates a comprehensive report (Task 6 - 20%) containing:

1. **Dataset Statistics**: Files, pages, characters, lines
2. **Task 1 Results**: Tokenization statistics, frequency distributions
3. **Task 2 Results**: Heaps' Law parameters, vocabulary growth analysis
4. **Task 3 Results**: BPE statistics, merge operations
5. **Task 4 Results**: Sentence segmentation statistics
6. **Task 5 Results**: Spell checker accuracy, test cases
7. **Extra Task Results**: Confusion matrix patterns, weighted accuracy

All reports are saved as:
- Individual task reports: `task{N}_report.txt`
- Overall summary: `summary.json`

## Key Features

- **Azerbaijani Language Support**: Handles special characters (É™, Ä±, Ã¶, Ã¼, ÄŸ, ÅŸ, Ã§)
- **Modular Design**: Each task is a separate, reusable module
- **Persistent Models**: All models can be saved and loaded
- **Interactive UI**: Beautiful Streamlit interface for testing
- **Comprehensive Testing**: Automatic generation of test cases
- **Detailed Reports**: Text and JSON format reports
- **Visualizations**: Matplotlib plots for Heaps' Law

## Technical Details

### Tokenization
- Pattern-based tokenization using regex
- Preserves Azerbaijani characters
- Case-insensitive processing

### Heaps' Law
- Non-linear curve fitting using scipy
- Logarithmic sampling for efficiency
- RÂ² goodness of fit calculation

### BPE
- Classic BPE algorithm implementation
- Character-level initialization
- End-of-word markers (</w>)

### Sentence Segmentation
- Rule-based segmentation
- Azerbaijani abbreviation handling
- Advanced punctuation support

### Spell Checking
- Dynamic programming Levenshtein distance
- Frequency-based ranking
- Configurable edit distance threshold

### Weighted Edit Distance
- Character confusion matrix learning
- Weighted dynamic programming
- Azerbaijani-specific confusion patterns

## Troubleshooting

**No models found in UI:**
```bash
# Run the main pipeline first
python main.py --sample --output ./outputs
```

**Module import errors:**
```bash
# Make sure all files are in the same directory
# Install dependencies
pip install -r requirements.txt
```

**Encoding errors with Azerbaijani characters:**
```bash
# Ensure your JSON files are UTF-8 encoded
# Python 3.6+ should handle this automatically
```

## Dependencies

- Python 3.7+
- numpy >= 1.21.0
- scipy >= 1.7.0
- matplotlib >= 3.4.0
- streamlit >= 1.28.0
- pandas >= 1.3.0

## Acknowledgments

- Azerbaijani language resources, mainly Presidential Library
- BPE algorithm by Sennrich et al.
- Heaps' Law by Harold Stanley Heaps
